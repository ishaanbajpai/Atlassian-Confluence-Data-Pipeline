<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agentic AI Technology Stack</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3, h4, h5, h6 {
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1em 0;
            overflow-x: auto;
            display: block;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
        }
        th {
            background-color: #f2f2f2;
        }
        img {
            max-width: 100%;
            height: auto;
        }
        figure {
            text-align: center;
            margin: 1em 0;
        }
        figcaption {
            font-style: italic;
            font-size: 0.9em;
            margin-top: 0.5em;
        }
        pre {
            background-color: #f5f5f5;
            padding: 10px;
            overflow-x: auto;
            border-radius: 3px;
            white-space: pre;
            font-family: monospace;
        }
        code {
            font-family: monospace;
            background-color: #f5f5f5;
            padding: 2px 4px;
            border-radius: 3px;
        }
        .code-block {
            background-color: #f5f5f5;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
            white-space: pre;
            font-family: monospace;
        }
        /* Syntax highlighting */
        .keyword { color: #0000ff; }
        .string { color: #a31515; }
        .comment { color: #008000; }
        .function { color: #795e26; }
        .number { color: #098658; }
        .type { color: #267f99; }
        .class { color: #267f99; }
        .variable { color: #001080; }
    </style>
</head>
<body>
    <h1>Agentic AI Technology Stack</h1>
    <div class="content">
        <p></p><h1><strong>1. Introduction</strong></h1><p>This document outlines the technical architecture and the technology choices made for the development of our AgenticAI. It provides detailed pros. for each technology selected, comparing them with available alternatives and explaining the technical advantages aligned with project requirements such as scalability, maintainability, real-time capabilities, and AI integration.</p><h1><strong>2. Frontend Stack</strong></h1><table ac:local-id="83bf6f25-926f-40f0-8ecd-9137e88f7787" data-layout="default" data-table-width="760"><tbody><tr><td><p><strong>Technology</strong></p></td><td><p><strong>Alternatives</strong></p></td><td><p><strong>Pros.</strong></p></td><td><p><strong>Comparison</strong></p></td></tr><tr><td data-highlight-colour="#c0c0c0"><p><strong>React 19</strong></p></td><td data-highlight-colour="#c0c0c0"><p>Angular, Vue, Svelte, SolidJS</p></td><td data-highlight-colour="#c0c0c0"><p>React 19 introduces significant performance improvements with React Compiler, enhanced Suspense for fine-grained async rendering, and modern features like Actions API. It supports future Server Components even though currently only client-side is used. Being the industry standard with a huge ecosystem, it ensures better onboarding, large community support, and compatibility with enterprise dev tools (e.g., Redux Toolkit, React Query, etc.). Also supports frameworks like Next.js if SSR is considered in future.</p></td><td data-highlight-colour="#c0c0c0"><p>Compared to <strong>Angular</strong> and <strong>Vue</strong>, <strong>React 19</strong> offers <strong>better performance optimizations</strong> with <strong>Enhanced Suspense</strong> and the <strong>React Compiler</strong>, providing a more <strong>scalable</strong> solution for complex, large-scale applications. <strong>Svelte</strong> and <strong>SolidJS</strong> may be faster in specific scenarios, but they lack the mature ecosystem and industry support that <strong>React 19</strong> provides. <strong>React</strong> also has <strong>better community support</strong> and more <strong>enterprise adoption</strong>, making it the safest long-term choice for frontend development.</p></td></tr><tr><td><p><strong>TypeScript</strong></p></td><td><p>JavaScript</p></td><td><p>Provides type safety; enhances tooling and debugging; reduces production bugs; essential for multi-team, large-scale development.</p></td><td><p>TypeScript offers <strong>early error detection</strong>, <strong>IDE support</strong>, and <strong>refactoring tools</strong> that JavaScript lacks, making it the preferred choice for <strong>maintainable enterprise applications</strong>.</p></td></tr><tr><td data-highlight-colour="#c0c0c0"><p><strong>Vite</strong></p></td><td data-highlight-colour="#c0c0c0"><p>Webpack</p></td><td data-highlight-colour="#c0c0c0"><p>Ultra-fast dev server with native ESM; faster HMR (Hot Module Replacement); minimal configuration; optimized production build process.</p></td><td data-highlight-colour="#c0c0c0"><p>Compared to Webpack, Vite provides a <strong>significantly faster dev experience</strong>, <strong>simpler configuration</strong>, and better <strong>tree-shaking</strong>, especially beneficial for modern frontend projects.</p></td></tr><tr><td><p><a data-card-appearance="inline" href="http://Socket.io">http://Socket.io</a> <strong> Client</strong></p></td><td><p>WebSocket API, SSE</p></td><td><p>Simplifies real-time, event-driven communication; automatic fallbacks for browser support; seamless with FastAPI WebSockets backend.</p></td><td><p><a data-card-appearance="inline" href="http://Socket.io">http://Socket.io</a>  provides <strong>reconnection logic</strong>, <strong>broadcast support</strong>, and <strong>fallbacks</strong> out of the box, which are not available with the raw WebSocket API or SSE, making it more reliable for complex apps.</p></td></tr><tr><td data-highlight-colour="#c0c0c0"><p><strong>SASS</strong></p></td><td data-highlight-colour="#c0c0c0"><p>Less, Stylus, PostCSS</p></td><td data-highlight-colour="#c0c0c0"><p>Extends CSS capabilities (variables, mixins, nesting); supports maintainable, scalable stylesheets; works well with Tailwind for customization.</p></td><td data-highlight-colour="#c0c0c0"><p>SASS has <strong>wider community adoption</strong>, <strong>better documentation</strong>, and <strong>stronger toolchain support</strong> than Less or Stylus, and complements Tailwind for advanced theming and overrides.</p></td></tr></tbody></table><h1><strong>3. Backend Stack</strong></h1><table ac:local-id="daeb5952-4067-4919-8354-27c7eb261676" data-layout="default" data-table-width="760"><tbody><tr><td><p><strong>Technology</strong></p></td><td><p><strong>Alternatives</strong></p></td><td><p><strong>Pros.</strong></p></td><td><p><strong>Comparison</strong></p></td></tr><tr><td data-highlight-colour="#c0c0c0"><p><strong>FastAPI</strong></p></td><td data-highlight-colour="#c0c0c0"><p>Flask, Django</p></td><td data-highlight-colour="#c0c0c0"><p>FastAPI is an asynchronous, high-performance web framework that leverages Starlette and Pydantic for asyncio support, automatic OpenAPI documentation, and data validation. Its key benefits include:</p><ul><li><p>Asynchronous Support: Built for high concurrency and non-blocking I/O, ideal for real-time applications like WebSocket-based communication.</p></li></ul><ul><li><p>High Performance: Optimized for speed, it handles requests faster than traditional frameworks like Flask or Django.</p></li></ul><ul><li><p>Type Safety: Automatic validation with Pydantic ensures consistent and reliable data handling.</p></li></ul><ul><li><p>OpenAPI &amp; Documentation: Generates interactive API docs for quick testing and integration.</p></li></ul><ul><li><p>Scalable &amp; Modular: Ideal for microservices and supporting features like OAuth and background tasks with minimal setup.</p></li></ul><p>FastAPI is perfect for building high-performance, scalable enterprise applications, offering ease of use, speed, and developer-friendly tools.</p></td><td data-highlight-colour="#c0c0c0"><p>Compared to <strong>Flask</strong> and <strong>Django</strong>, FastAPI is far superior in terms of <strong>async/await support</strong>, making it ideal for applications requiring <strong>real-time</strong> capabilities and <strong>scalability</strong>. While <strong>Express.js</strong> and <strong>Sanic</strong> are fast and lightweight, FastAPI offers superior <strong>automatic documentation</strong>, reducing the time spent on creating and maintaining API docs. Unlike Django, FastAPI is much <strong>faster</strong> due to its <strong>async-first architecture</strong>, making it ideal for applications with high throughput and low latency requirements.</p></td></tr><tr><td><p><strong>SQLAlchemy</strong></p></td><td><p>cx_Oracle, Django ORM, Tortoise ORM</p></td><td><p>Mature Python ORM; clean separation of models and queries; native Oracle support; async capabilities improving rapidly.</p></td><td><p>Compared to Django ORM (tightly coupled with Django) and Tortoise (less mature), SQLAlchemy offers <strong>flexibility</strong>, <strong>fine-grained control</strong>, and <strong>strong Oracle support</strong>, making it suitable for complex enterprise use cases.</p></td></tr><tr><td data-highlight-colour="#c0c0c0"><p><strong>Oracle 23AI Database</strong></p></td><td data-highlight-colour="#c0c0c0"><p>PostgreSQL + pgvector, MySQL, MongoDB, Azure Cosmos DB</p></td><td data-highlight-colour="#c0c0c0"><p>Combines traditional RDBMS with native vector storage; supports PL/SQL; ideal for secure, enterprise-grade AI-enhanced applications.</p></td><td data-highlight-colour="#c0c0c0"><p>Oracle 23AI uniquely integrates <strong>AI-native features (like vector indexing)</strong> into a proven RDBMS, unlike PostgreSQL or Cosmos DB which require add-ons. It's <strong>enterprise-hardened</strong>, secure, and scalable.</p></td></tr><tr><td><p><strong>Redis</strong></p></td><td><p>Memcached, Kafka (for pub/sub), RabbitMQ</p></td><td><p>In-memory key-value store; ultra-fast caching; supports pub/sub and lightweight queuing; crucial for real-time updates and session storage.</p></td><td><p>Redis offers <strong>multi-purpose support</strong> (cache + pub/sub + streams), unlike Memcached (cache-only) or RabbitMQ/Kafka (heavier infra). It's lightweight yet powerful for <strong>low-latency applications</strong>.</p></td></tr><tr><td data-highlight-colour="#c0c0c0"><p><strong>Python 3.10+</strong></p></td><td data-highlight-colour="#c0c0c0"><p>Node.js, Go, Java, .NET</p></td><td data-highlight-colour="#c0c0c0"><p>Modern syntax features (e.g., structural pattern matching); async/await support; rich AI/ML ecosystem; ideal for rapid prototyping and scalability.</p></td><td data-highlight-colour="#c0c0c0"><p>Python balances <strong>developer speed</strong>, <strong>rich AI libraries</strong>, and <strong>async capabilities</strong>, unlike Java/.NET (more verbose) or Go (low-level), making it ideal for AI-driven backend services.</p></td></tr><tr><td><p><strong>Poetry</strong></p></td><td><p>pip + venv, Pipenv, Conda</p></td><td><p>Modern dependency and package management; lockfiles ensure reproducible builds; better suited for enterprise and monorepo structures.</p></td><td><p>Poetry provides <strong>dependency resolution + packaging</strong> in one tool, unlike pip + venv or Pipenv. It's more deterministic and CI/CD friendly than Conda for Python-only projects.</p></td></tr><tr><td data-highlight-colour="#c0c0c0"><p><strong>RabbitMQ</strong></p></td><td data-highlight-colour="#c0c0c0"><p>Kafka, Redis Streams, Amazon SQS, ActiveMQ</p></td><td data-highlight-colour="#c0c0c0"><p>RabbitMQ is a reliable message broker designed for asynchronous communication and decoupling systems. Key benefits include:</p><ul><li><p>Reliability: Ensures message durability, preventing data loss even during failures.</p></li></ul><ul><li><p>Flexible Routing: Supports advanced AMQP routing for various messaging patterns (e.g., publish/subscribe).</p></li></ul><ul><li><p>Scalability: Scales horizontally to handle high-throughput, distributed architectures.</p></li></ul><ul><li><p>Protocol Support: Offers compatibility with widely used AMQP for broad integration with other systems.</p></li></ul><p>RabbitMQ is perfect for building scalable, decoupled systems that require efficient and reliable message processing.</p></td><td data-highlight-colour="#c0c0c0"><p>Compared to Kafka, which excels in <strong>high-throughput log streaming</strong>, <strong>RabbitMQ</strong> is better suited for <strong>task-based messaging</strong> with complex routing needs. Unlike Redis Streams or SQS, RabbitMQ provides richer delivery guarantees (e.g., <strong>acknowledgements, retries, dead-lettering</strong>) and supports <strong>pluggable exchanges</strong> and <strong>message filtering</strong>. It also offers better <strong>visibility into message queues</strong> for debugging and monitoring, which is critical in microservices and enterprise applications.</p></td></tr></tbody></table><h1><strong>4. Generative AI Stack</strong></h1><table ac:local-id="812ca502-0689-4df3-b91b-7035f1a3bb0e" data-layout="default" data-table-width="760"><tbody><tr><td><p><strong>Technology</strong></p></td><td><p><strong>Alternatives</strong></p></td><td><p><strong>Pros.</strong></p></td><td><p><strong>Comparison</strong></p></td></tr><tr><td data-highlight-colour="#c0c0c0"><p><strong>LangGraph</strong></p></td><td data-highlight-colour="#c0c0c0"><p>Haystack Agents, AutoGen, CrewAI</p></td><td data-highlight-colour="#c0c0c0"><p>LangGraph enables construction of stateful, directed, multi-agent workflows using a DAG (Directed Acyclic Graph) architecture. Each node can represent an agent, tool, or decision step, and edges define transitions based on logic, memory, or external signals. It's particularly suitable for enterprise systems where complex decision flows, retries, and streaming outputs are needed. LangGraph’s design promotes modular, testable, and observable pipelines, which is critical in production AI systems involving dynamic response routing and fallback strategies.</p></td><td data-highlight-colour="#c0c0c0"><p>Compared to <strong>Haystack Agents</strong> and <strong>AutoGen</strong>, <strong>LangGraph</strong> provides better support for <strong>stateful workflows</strong> and <strong>complex decision-making</strong>, offering a clear advantage for enterprises dealing with complex interactions and the need for <strong>reliable retry logic</strong> and <strong>streaming</strong>. While <strong>CrewAI</strong> is also designed for multi-agent workflows, LangGraph’s use of <strong>DAG</strong> and integration of modular and testable components gives it a more robust and flexible architecture for production-grade applications.</p></td></tr><tr><td><p><strong>Therix</strong></p></td><td><p>LangFuse, PromptLayer, WhyLabs, Arize AI</p></td><td><p>Therix specializes in observability and prompt management, providing robust tools for AI systems management.</p><ul><li><p>Observability: Offers real-time monitoring to track model performance, AI agent interactions, and system health, ensuring proactive issue resolution and continuous optimization.</p></li></ul><ul><li><p>Prompt Management: Provides centralized tools to manage, test, and version AI prompts, making it easy to ensure consistency, scalability, and efficiency in prompt usage across multiple applications.</p></li></ul></td><td><p>While tools like LangFuse and PromptLayer offer observability and prompt tracking, <strong>Therix gives us full code-level control</strong> since it is developed in-house. This allows deep customization tailored to our enterprise needs, seamless integration into our stack, and tighter data governance. Competing tools may offer similar features but are limited in extensibility, cost-effectiveness, or private hosting flexibility. Therix also aligns better with internal compliance and security requirements.</p></td></tr><tr><td data-highlight-colour="#c0c0c0"><p><strong>Model Context Protocol (MCP)</strong></p></td><td data-highlight-colour="#c0c0c0"><p>Custom API-based Context Management, Memory Networks</p></td><td data-highlight-colour="#c0c0c0"><p>MCP (Model Context Protocol) is a specialized protocol designed to manage context during interactions with AI models. It enables persistent context storage, which allows the system to track user sessions, conversation history, and other dynamic context attributes that influence AI outputs. MCP ensures that the model can refer back to previous interactions or system states, which is critical in multi-turn conversations or complex workflows. By using MCP, an AI model can be more context-aware, leading to more accurate and personalized responses. It also ensures scalability in multi-agent or multi-user environments by decoupling the AI model from direct session handling, allowing the system to scale across multiple parallel sessions while maintaining context integrity. MCP is vital in enterprise AI applications where state consistency and data privacy are paramount. It can be integrated with existing AI pipelines and can be used to update or retrieve context as needed, facilitating complex agent-based workflows and more robust decision-making in real-time.</p></td><td data-highlight-colour="#c0c0c0"><p>Compared to <strong>Custom API-based Context Management</strong>, MCP offers more <strong>standardized</strong> and <strong>scalable</strong> solutions by maintaining context in a structured and persistent manner. <strong>Memory Networks</strong> can also manage context, but they typically struggle with scalability and <strong>real-time decision-making</strong> in high-load environments. MCP is more efficient and resilient for <strong>enterprise-scale AI systems</strong>, supporting <strong>complex workflows</strong> with better <strong>data privacy</strong> and <strong>session integrity</strong>.</p></td></tr><tr><td><p><strong>AWS Bedrock</strong></p></td><td><p>Azure OpenAI, Google Vertex AI, Hugging Face Endpoints</p></td><td><p>AWS Bedrock provides access to multiple foundation models (Anthropic Claude, Mistral, Amazon Titan, Cohere, etc.) via a <strong>unified API</strong>, removing the need to manage infrastructure. Supports <strong>RAG</strong>, <strong>Agents</strong>, <strong>Guardrails</strong>, and <strong>model chaining</strong>. Integration with IAM, VPCs, Step Functions, and Lambda makes it ideal for building secure, scalable GenAI systems in regulated environments.</p></td><td><p><strong>AWS Bedrock</strong> stands out due to its <strong>wide range of model choices</strong> and the ability to seamlessly integrate with AWS services like <strong>IAM</strong>, <strong>VPCs</strong>, and <strong>Lambda</strong>. Compared to <strong>Google Vertex AI</strong> and <strong>Azure OpenAI</strong>, AWS Bedrock simplifies model management with its <strong>unified API</strong> and robust security features, making it more suitable for organizations operating in regulated or enterprise environments. <strong>Hugging Face Endpoints</strong> also offers model hosting but lacks the same level of integrated security and service scalability offered by AWS Bedrock.</p></td></tr><tr><td data-highlight-colour="#c0c0c0"><p><strong>Amazon SageMaker</strong></p></td><td data-highlight-colour="#c0c0c0"><p>Vertex AI, Azure ML, Databricks</p></td><td data-highlight-colour="#c0c0c0"><p>SageMaker enables end-to-end ML workflows including data preprocessing, model training, tuning, deployment, and monitoring. It supports secure custom LLM hosting with GPU acceleration, fine-tuning on domain-specific datasets, and scalable deployment via endpoints. Its integration with S3, EventBridge, and Step Functions makes it well-suited for building robust enterprise-grade GenAI pipelines.</p></td><td data-highlight-colour="#c0c0c0"><p><strong>SageMaker</strong> provides a <strong>comprehensive, fully-managed platform</strong> for ML workflows, with the added advantage of deep integration with other AWS services like <strong>S3</strong>, <strong>EventBridge</strong>, and <strong>Step Functions</strong>. Compared to <strong>Vertex AI</strong> and <strong>Azure ML</strong>, SageMaker offers more advanced fine-tuning capabilities and a wider range of deployment options, including custom LLM hosting with GPU acceleration. While <strong>Databricks</strong> excels in collaborative data science environments, SageMaker is a better fit for production-level GenAI systems that require a more integrated and scalable approach to deployment and monitoring.</p></td></tr></tbody></table><p></p><h1><strong>4. Generative AI Models</strong></h1><table ac:local-id="a6f7e777-4757-4917-9176-521ec12ee3e8" data-layout="default" data-table-width="760"><tbody><tr><td><p><strong>Model Name</strong></p></td><td><p><strong>Alternatives</strong></p></td><td><p><strong>Pros.</strong></p></td><td><p><strong>Comparison</strong></p></td></tr><tr><td data-highlight-colour="#c0c0c0"><p><strong>Llama 3.3 70B</strong></p></td><td data-highlight-colour="#c0c0c0"><p>GPT-4, Claude 3, Gemini 1.5, Falcon 180B</p></td><td data-highlight-colour="#c0c0c0"><p>Llama 3.3 70B is a multilingual, instruction-tuned, text-only large language model developed by Meta. It offers enhanced performance in reasoning, mathematics, general knowledge, instruction following, and tool use. Key features include:</p><ul><li><p>Multilingual Support: Natively supports English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.</p></li></ul><ul><li><p>Extended Context Window: Capable of processing up to 128,000 tokens, facilitating long-form content generation and complex document analysis.</p></li></ul><ul><li><p>Comparable Performance: Delivers performance on par with Llama 3.1 405B while requiring significantly fewer computational resources.</p></li></ul><ul><li><p>Instruction Tuning: Fine-tuned using Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF) to align with human preferences for helpfulness and safety.</p></li></ul><ul><li><p>Open Access: Available under the Llama 3.3 Community License Agreement, promoting transparency and accessibility for research and commercial use.</p></li></ul><p>Use Cases: Ideal for applications requiring advanced reasoning, multilingual capabilities, and efficient processing, such as enterprise chatbots, content generation, and multilingual customer support systems.</p></td><td data-highlight-colour="#c0c0c0"><p>Unlike GPT-4, Claude 3, or Gemini, <strong>Llama 3.3 70B is open-access</strong>, providing full control over deployment, fine-tuning, and on-prem hosting—crucial for compliance and cost control in enterprise environments. While closed models excel in raw performance, Llama 3.3 offers near-parity in benchmarks, with significantly reduced infrastructure requirements compared to Falcon 180B. Its native multilingual support and alignment training make it highly usable out-of-the-box for enterprise LLM applications.</p></td></tr></tbody></table><p></p>
    </div>
</body>
</html>